# -*- coding: utf-8 -*-
import os
import re
import json
import time
import math
import random
import traceback
import unicodedata
from datetime import datetime, timedelta, date
from concurrent.futures import ThreadPoolExecutor
from collections import defaultdict  # ‚úÖ d√πng cho smart-dedup

import pandas as pd

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, TimeoutException


# ======================
# ƒê∆Ø·ªúNG D·∫™N L∆ØU (gi·ªëng crawl_ticket / crawl_facility)
# ======================
DEFAULT_WIN_BASE = r"C:\KLTN2\data\raw"
LOCAL_RAW_DIR = os.getenv("LOCAL_RAW_DIR", DEFAULT_WIN_BASE)
REVIEW_BASE_DIR = os.path.join(LOCAL_RAW_DIR, "review")
MASTER_PATH = os.path.join(REVIEW_BASE_DIR, "bus_reviews_master.jsonl")

# S·ªë ng√†y g·∫ßn nh·∫•t ƒë·ªÉ l·∫•y review (m·∫∑c ƒë·ªãnh 7; c√≥ th·ªÉ override b·∫±ng env)
REVIEW_LOOKBACK_DAYS = int(os.getenv("REVIEW_LOOKBACK_DAYS", "7"))


# ======================
# Selenium Driver
# ======================
def initialize_driver():
    options = webdriver.ChromeOptions()
    flags = [
        "--headless=new", "--no-sandbox", "--disable-dev-shm-usage",
        "--disable-gpu", "--disable-extensions",
        "--disable-blink-features=AutomationControlled",
        "--window-size=1920,1080",
        "--disable-notifications", "--disable-popup-blocking"
    ]
    for f in flags:
        options.add_argument(f)
    # ·ªîn ƒë·ªãnh h∆°n khi ch·∫°y nhi·ªÅu tab
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option("useAutomationExtension", False)
    driver = webdriver.Chrome(options=options)
    return driver


def safe_click(driver, elem):
    """Scroll element v√†o gi·ªØa + th·ª≠ click b√¨nh th∆∞·ªùng, n·∫øu l·ªói th√¨ d√πng JS click."""
    try:
        driver.execute_script("arguments[0].scrollIntoView({block:'center'});", elem)
        time.sleep(0.1)
        elem.click()
    except Exception:
        driver.execute_script("arguments[0].click();", elem)


def scroll_and_click_see_more(driver):
    """Scroll v√† b·∫•m 'See more' ƒë·ªÉ t·∫£i h·∫øt k·∫øt qu·∫£ tr√™n trang listing."""
    previous_count = 0
    while True:
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(0.6)
        bus_elements = driver.find_elements(By.CLASS_NAME, "bus-name")
        current_count = len(bus_elements)

        if current_count == previous_count:
            # Th·ª≠ t√¨m n√∫t 'load more' l·∫ßn cu·ªëi
            try:
                button_xpath = ("//button[contains(@class, 'load-more') or "
                                "contains(.,'Xem th√™m') or contains(.,'See more')]")
                btn = WebDriverWait(driver, 2).until(
                    EC.element_to_be_clickable((By.XPATH, button_xpath))
                )
                safe_click(driver, btn)
                time.sleep(0.6)
            except Exception:
                break
        previous_count = current_count


def get_bus_names_and_buttons(driver):
    """L·∫•y danh s√°ch bus: t√™n + n√∫t chi ti·∫øt."""
    wait = WebDriverWait(driver, 6)
    bus_data = []
    try:
        wait.until(EC.presence_of_all_elements_located((By.CLASS_NAME, "bus-name")))
        ticket_containers = driver.find_elements(By.XPATH, "//div[contains(@class, 'ticket')]")
        for container in ticket_containers:
            try:
                bus_name_element = container.find_element(By.CLASS_NAME, "bus-name")
                bus_name = (bus_name_element.text or "").strip()
                detail_button = container.find_element(By.XPATH, ".//button[contains(@class, 'btn-detail')]")
                if bus_name and detail_button:
                    bus_data.append({"name": bus_name, "button": detail_button})
            except NoSuchElementException:
                continue
        print(f"Found {len(bus_data)} bus entries")
        return bus_data
    except Exception as e:
        print(f"Error getting bus names: {e}")
        return []


# ======================
# Helpers: cu·ªôn panel review & parse ng√†y ti·∫øng Vi·ªát
# ======================
def _scroll_reviews_panel(driver):
    """
    Cu·ªôn v√πng review trong ant-drawer n·∫øu c√≥; n·∫øu kh√¥ng th√¨ cu·ªôn window.
    Tr·∫£ v·ªÅ True n·∫øu ƒë√£ cu·ªôn panel; False n·∫øu fallback cu·ªôn window.
    """
    panel_xpaths = [
        "//div[contains(@class,'ant-drawer-open')]//div[contains(@class,'ant-drawer-body')]",
        "//div[contains(@class,'review')]/ancestor::div[contains(@class,'ant-drawer-body')]",
    ]
    for xp in panel_xpaths:
        els = driver.find_elements(By.XPATH, xp)
        if els:
            panel = els[0]
            driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight;", panel)
            return True
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    return False


def _strip_accents(s: str) -> str:
    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')


def parse_vn_date(raw: str) -> date | None:
    """
    H·ªó tr·ª£ c√°c d·∫°ng:
    - dd/MM/yyyy | dd-MM-yyyy
    - 'H√¥m nay' | 'H√¥m qua'
    - 'x ng√†y/gi·ªù/ph√∫t tr∆∞·ªõc'
    - 'dd th√°ng MM, yyyy' | 'dd thg MM, yyyy'
    """
    if not raw:
        return None
    txt = raw.strip().lower()
    today = date.today()
    txt_norm = _strip_accents(txt)

    # H√¥m nay / H√¥m qua
    if "hom nay" in txt_norm:
        return today
    if "hom qua" in txt_norm:
        return today - timedelta(days=1)

    # X ng√†y/gi·ªù/ph√∫t tr∆∞·ªõc
    m = re.search(r"(\d+)\s*(ngay|gio|phut)\s*truoc", txt_norm)
    if m:
        val = int(m.group(1))
        unit = m.group(2)
        if unit == "ngay":
            return today - timedelta(days=val)
        if unit == "gio":
            # Quy ƒë·ªïi th√¥ theo ng√†y ƒë·ªÉ l·ªçc (24h ~ 1 ng√†y)
            days = max(0, math.floor(val / 24))
            return today - timedelta(days=days)
        if unit == "phut":
            return today

    # dd/MM/yyyy ho·∫∑c dd-MM-yyyy
    for fmt in ("%d/%m/%Y", "%d-%m-%Y"):
        try:
            return datetime.strptime(txt, fmt).date()
        except Exception:
            pass

    # "dd th√°ng MM, yyyy" | "dd thg MM, yyyy" (kh√¥ng d·∫•u)
    m2 = re.search(r"(\d{1,2})\s*(thang|thg)\s*(\d{1,2})[, ]+\s*(\d{4})", txt_norm)
    if m2:
        d = int(m2.group(1))
        mth = int(m2.group(3))
        y = int(m2.group(4))
        try:
            return date(y, mth, d)
        except Exception:
            return None

    return None


# ======================
# Parse & Extract Reviews
# ======================
def extract_reviews_from_page(driver, start_date, end_date):
    """
    Tr√≠ch review ·ªü trang hi·ªán t·∫°i, ch·ªâ gi·ªØ Date trong [start_date, end_date].
    Tr·∫£ v·ªÅ list[dict]: customer_name, stars, comment, Date (ISO).
    """
    reviews = []
    processed = set()

    try:
        # M·ªÅm d·∫ªo h∆°n: container c√≥ class 'review' (kh√¥ng ch·ªâ 'review-item')
        WebDriverWait(driver, 4).until(
            EC.presence_of_element_located((By.XPATH, "//div[contains(@class,'review')]"))
        )
        review_containers = driver.find_elements(By.XPATH, "//div[contains(@class,'review')]")
        print(f"Found {len(review_containers)} review containers on current page")

        for container in review_containers:
            try:
                # T√™n KH
                customer_name = "Unknown"
                try:
                    customer_name_elem = container.find_element(
                        By.XPATH, ".//*[contains(@class,'name')] | .//p[contains(@class,'name')]"
                    )
                    customer_name = (customer_name_elem.text or "").strip()
                except NoSuchElementException:
                    pass

                # S·ªë sao
                stars = 0
                try:
                    stars = len(container.find_elements(By.XPATH, ".//i[contains(@class,'color--critical')]"))
                    if stars == 0:
                        aria = container.get_attribute("aria-label") or ""
                        mstar = re.search(r"(\d+(\.\d+)?)", aria)
                        if mstar:
                            stars = int(float(mstar.group(1)))
                except Exception:
                    pass

                # Comment
                comment = ""
                try:
                    comment_elem = container.find_element(
                        By.XPATH, ".//*[contains(@class,'comment')] | .//p[contains(@class,'comment')]"
                    )
                    comment = (comment_elem.text or "").strip()
                except NoSuchElementException:
                    pass

                # Ng√†y ƒë√°nh gi√° (raw & parsed)
                raw_date = ""
                try:
                    date_elem = container.find_element(
                        By.XPATH, ".//*[contains(@class,'rated-date') or contains(@class,'date')]"
                    )
                    raw_date = (date_elem.text or "").strip()
                except NoSuchElementException:
                    pass

                d = parse_vn_date(raw_date)
                if not d:
                    # N·∫øu kh√¥ng parse ƒë∆∞·ª£c, ƒë·ª´ng lo·∫°i‚Äîg√°n h√¥m nay ƒë·ªÉ b·∫°n c√≥ data tham kh·∫£o
                    d = date.today()
                if not (start_date <= d <= end_date):
                    continue

                key = (customer_name, stars, comment, d.isoformat())
                if key in processed:
                    continue
                processed.add(key)

                # ‚úÖ CH·ªàNH #1: L∆∞u Date ISO thay v√¨ raw
                reviews.append({
                    "customer_name": customer_name,
                    "stars": stars,
                    "comment": comment,
                    "Date": d.isoformat()
                })

            except Exception as e:
                print(f"Error extracting single review: {e}")
                traceback.print_exc()

        return reviews

    except Exception as e:
        print(f"Error extracting reviews from page: {e}")
        traceback.print_exc()
        return []


def extract_reviews_for_bus(driver, bus_entry):
    """V√†o chi ti·∫øt t·ª´ng bus, chuy·ªÉn tab REVIEW, ph√¢n trang ƒë·ªÉ l·∫•y review."""
    bus_name = bus_entry["name"]
    all_reviews = []
    wait = WebDriverWait(driver, 6)
    page_number = 1

    # Kho·∫£ng ng√†y: t·ª´ (today - LOOKBACK) -> today
    end_date = datetime.now().date()
    start_date = end_date - timedelta(days=REVIEW_LOOKBACK_DAYS)

    try:
        safe_click(driver, bus_entry["button"])

        # Tab REVIEW
        try:
            review_tab = wait.until(EC.element_to_be_clickable((By.XPATH, "//div[@role='tab' and @id='REVIEW']")))
            safe_click(driver, review_tab)
            time.sleep(0.6)
        except Exception:
            # Th·ª≠ ph∆∞∆°ng √°n d·ª± ph√≤ng
            tabs = driver.find_elements(By.XPATH, "//div[@role='tab']")
            clicked = False
            for tab in tabs:
                if "REVIEW" in (tab.get_attribute("id") or "") or "ƒê√ÅNH GI√Å" in tab.text:
                    safe_click(driver, tab)
                    time.sleep(0.6)
                    clicked = True
                    break
            if not clicked:
                try:
                    safe_click(driver, bus_entry["button"])  # ƒë√≥ng panel
                except Exception:
                    pass
                return []

        print(f"Extracting reviews for '{bus_name}'")

        try:
            wait.until(EC.presence_of_element_located((By.XPATH, "//div[contains(@class, 'review')]")))
        except TimeoutException:
            print(f"  No reviews found for '{bus_name}'")
            try:
                safe_click(driver, bus_entry["button"])  # ƒë√≥ng panel
            except Exception:
                pass
            time.sleep(0.2)
            return []

        while True:
            _scroll_reviews_panel(driver)
            time.sleep(0.4 + random.random() * 0.3)

            page_reviews = extract_reviews_from_page(driver, start_date, end_date)
            for r in page_reviews:
                r["bus_name"] = bus_name
            all_reviews.extend(page_reviews)
            print(f"  Page {page_number}: Extracted {len(page_reviews)} reviews")

            # Pagination trong drawer
            try:
                next_btn = WebDriverWait(driver, 3).until(
                    EC.element_to_be_clickable((By.XPATH,
                        "//div[contains(@class,'ant-drawer-open')]//li[contains(@class,'ant-pagination-next')]"
                    ))
                )
                if next_btn.get_attribute("aria-disabled") == "true":
                    break
                safe_click(driver, next_btn)
                page_number += 1
                time.sleep(0.6 + random.random() * 0.4)
            except Exception:
                break

        # ƒë√≥ng panel
        try:
            safe_click(driver, bus_entry["button"])
        except Exception:
            pass
        time.sleep(0.15)

        print(f"  Total reviews collected for '{bus_name}': {len(all_reviews)}")
        return all_reviews

    except Exception as e:
        print(f"Error extracting reviews for {bus_name}: {e}")
        traceback.print_exc()
        try:
            safe_click(driver, bus_entry["button"])
        except Exception:
            pass
        time.sleep(0.2)
        return []


# ======================
# Company IDs
# ======================
def get_company_id(province, key, driver, date_str_dd_mm_yyyy):
    """
    L·∫•y danh s√°ch (bus_name, company_id) cho 1 t·ªânh/ng√†y.
    date_str ph·∫£i l√† dd-mm-YYYY (ƒë√∫ng format URL Vexere).
    """
    url = f"https://vexere.com/vi-VN/ve-xe-khach-tu-sai-gon-di-{province}-{key}.html?date={date_str_dd_mm_yyyy}"
    driver.get(url)
    try:
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, "bus-name")))
        scroll_and_click_see_more(driver)
    except Exception:
        print(f"Couldn't load data from {url}")
        return []

    ids = []
    containers = driver.find_elements(By.CSS_SELECTOR, "[data-company-id]")
    names = [b.text.strip() for b in driver.find_elements(By.CLASS_NAME, "bus-name")]
    comp_ids = [c.get_attribute("data-company-id") or "Unknown" for c in containers]
    n = min(len(names), len(comp_ids))
    for i in range(n):
        ids.append([names[i], comp_ids[i]])
    return ids


def process_company(province, key, company_id, date_str_dd_mm_yyyy):
    """X·ª≠ l√Ω t·ª´ng company_id (m·ªôt driver/lu·ªìng)."""
    driver = initialize_driver()
    all_reviews = []
    try:
        print(f"\nProcessing company ID: {company_id} in province: {province}")
        company_url = (
            f"https://vexere.com/vi-VN/ve-xe-khach-tu-sai-gon-di-{province}-{key}.html"
            f"?date={date_str_dd_mm_yyyy}&companies={company_id}&sort=time%3Aasc"
        )
        driver.get(company_url)
        time.sleep(1.2)
        entries = get_bus_names_and_buttons(driver)
        for e in entries:
            all_reviews.extend(extract_reviews_for_bus(driver, e))
            time.sleep(0.3 + random.random() * 0.7)  # nh·∫π nh√†ng ƒë·ªÉ tr√°nh b·ªã ch·∫∑n
    except Exception as e:
        print(f"Error processing company {company_id}: {e}")
        traceback.print_exc()
    finally:
        driver.quit()
    return all_reviews


# ======================
# L∆ØU & MERGE JSONL (gi·ªëng crawl_facility)
# ======================
def _ensure_review_dirs():
    os.makedirs(REVIEW_BASE_DIR, exist_ok=True)
    today = datetime.now().strftime("%Y-%m-%d")
    daily_dir = os.path.join(REVIEW_BASE_DIR, f"date={today}")
    os.makedirs(daily_dir, exist_ok=True)
    return daily_dir, today


def _normalize_reviews_df(df: pd.DataFrame) -> pd.DataFrame:
    """
    Chu·∫©n ho√° schema & ki·ªÉu d·ªØ li·ªáu.
    Output columns: Review_Id (optional), Bus_Name, Customer_Name, Stars, Comment, Date
    """
    out = df.copy()
    for c, default in [
        ("Review_Id", None),
        ("Bus_Name", "Unknown"),
        ("Customer_Name", "Unknown"),
        ("Stars", 0),
        ("Comment", ""),
        ("Date", "Unknown"),
    ]:
        if c not in out.columns:
            out[c] = default

    out["Bus_Name"] = out["Bus_Name"].astype(str)
    out["Customer_Name"] = out["Customer_Name"].astype(str)
    out["Comment"] = out["Comment"].astype(str)
    out["Stars"] = pd.to_numeric(out["Stars"], errors="coerce").fillna(0).astype(int)
    return out[["Review_Id", "Bus_Name", "Customer_Name", "Stars", "Comment", "Date"]]


def _save_daily_jsonl(df: pd.DataFrame):
    daily_dir, today = _ensure_review_dirs()
    out = _normalize_reviews_df(df)
    daily_path = os.path.join(daily_dir, f"bus_reviews_{today}.jsonl")
    out.to_json(daily_path, orient="records", lines=True, force_ascii=False)
    print(f"üíæ ƒê√£ l∆∞u file NG√ÄY (JSONL): {daily_path}")


def _read_master_jsonl() -> pd.DataFrame:
    if not os.path.exists(MASTER_PATH):
        return pd.DataFrame(columns=["Review_Id", "Bus_Name", "Customer_Name", "Stars", "Comment", "Date"])
    try:
        return pd.read_json(MASTER_PATH, orient="records", lines=True, dtype={"Review_Id": "Int64"})
    except ValueError:
        # file r·ªóng/tr·ªëng
        return pd.DataFrame(columns=["Review_Id", "Bus_Name", "Customer_Name", "Stars", "Comment", "Date"])


def _merge_to_master_jsonl(df_new: pd.DataFrame):
    """
    Merge v√†o master (JSONL) v·ªõi Review_Id tƒÉng d·∫ßn.
    Tr√πng l·∫∑p ƒë∆∞·ª£c lo·∫°i theo (Bus_Name, Customer_Name, Comment, Date, Stars).
    """
    master = _read_master_jsonl()
    max_id = int(master["Review_Id"].max()) if not master.empty and master["Review_Id"].notna().any() else 0

    df_new = _normalize_reviews_df(df_new)

    # Lo·∫°i tr√πng v·ªõi master theo kh√≥a
    key_cols = ["Bus_Name", "Customer_Name", "Comment", "Date", "Stars"]
    if not master.empty:
        master_keys = set(tuple(row) for row in master[key_cols].itertuples(index=False, name=None))
        mask_keep = ~df_new[key_cols].apply(tuple, axis=1).isin(master_keys)
        df_new = df_new.loc[mask_keep].copy()

    # C·∫•p Review_Id cho nh·ªØng b·∫£n ghi ch∆∞a c√≥
    need_id_mask = df_new["Review_Id"].isna() | (df_new["Review_Id"].astype(str).str.strip() == "")
    if need_id_mask.any():
        count_need = int(need_id_mask.sum())
        new_ids = list(range(max_id + 1, max_id + 1 + count_need))
        df_new.loc[need_id_mask, "Review_Id"] = new_ids
        max_id += count_need

    df_new["Review_Id"] = df_new["Review_Id"].astype(int)

    # G·ªôp v√† ghi ƒë√® master
    updated = pd.concat([master, df_new], ignore_index=True)
    updated.to_json(MASTER_PATH, orient="records", lines=True, force_ascii=False)
    print(f"‚úÖ ƒê√£ MERGE v√†o MASTER (JSONL): {MASTER_PATH} ‚Äî t·ªïng {len(updated)} d√≤ng")


# ======================
# H√ÄM CH√çNH
# ======================
def crwl_reviews():
    provinces_keys = {
        # B·∫°n c√≥ th·ªÉ b·∫≠t/t·∫Øt b·ªõt t·ªânh ƒë·ªÉ th·ª≠ ·ªïn ƒë·ªãnh
        "binh-thuan": "129t1111",
        "binh-dinh": "129t181",
    }

    # Vexere ·ªïn h∆°n khi query ng√†y mai -> PH·∫¢I l√† dd-mm-YYYY
    query_date = (datetime.now() + timedelta(days=1)).strftime("%d-%m-%Y")

    collected = []
    processed_company_ids = set()

    # 2 thread cho Windows ·ªïn ƒë·ªãnh h∆°n (gi·∫£m QUOTA_EXCEEDED)
    with ThreadPoolExecutor(max_workers=2) as executor:
        futures = []
        for province, key in provinces_keys.items():
            print(f"\n--- Processing province: {province} ---")
            # L·∫•y danh s√°ch company_id tr∆∞·ªõc
            driver = initialize_driver()
            try:
                pairs = get_company_id(province, key, driver, query_date)
            finally:
                driver.quit()

            uniq_ids = sorted(set([p[1] for p in pairs if len(p) == 2]))
            for cid in uniq_ids:
                if cid in processed_company_ids:
                    print(f"Skipping already processed company ID: {cid}")
                    continue
                processed_company_ids.add(cid)
                futures.append(executor.submit(process_company, province, key, cid, query_date))

        # Thu k·∫øt qu·∫£
        for f in futures:
            try:
                company_reviews = f.result()
                collected.extend(company_reviews)
            except Exception as e:
                print(f"Thread error: {e}")

    # ‚úÖ SMART-DEDUP: ch·ªâ gi·ªØ d√≤ng ƒë·∫ßy ƒë·ªß nh·∫•t theo (Bus_Name, Comment, Date)
    grouped = defaultdict(list)
    for r in collected:
        bn = r.get("bus_name", "Unknown")
        cn = r.get("customer_name", "Unknown")
        cm = r.get("comment", "")
        dt = r.get("Date") or date.today().isoformat()   # d√πng Date ISO ƒë√£ l∆∞u
        st = int(r.get("stars", 0) or 0)
        rid = r.get("review_id") or r.get("Review_Id") or 0  # tie-break n·∫øu c√≥

        grouped[(bn, cm, dt)].append({
            "Customer_Name": cn,
            "Stars": st,
            "Review_Id": rid
        })

    deduped = []
    for (bn, cm, dt), rows in grouped.items():
        # S·∫Øp x·∫øp theo m·ª©c "ƒë·∫ßy ƒë·ªß": Stars>0 ‚Üí t√™n != Unknown ‚Üí t√™n d√†i h∆°n ‚Üí Review_Id nh·ªè h∆°n
        rows_sorted = sorted(
            rows,
            key=lambda x: (
                x["Stars"] > 0,
                (str(x["Customer_Name"]).lower() != "unknown"),
                len(str(x["Customer_Name"])),
                -(int(x["Review_Id"]) if str(x["Review_Id"]).isdigit() else 0)  # ƒë·∫£o d·∫•u ƒë·ªÉ Review_Id nh·ªè ∆∞u ti√™n (·ªü sort reverse=False)
            ),
            reverse=True
        )
        best = rows_sorted[0]
        deduped.append({
            "Bus_Name": bn,
            "Customer_Name": best["Customer_Name"],
            "Stars": int(best["Stars"]),
            "Comment": cm,
            "Date": dt
        })

    df = pd.DataFrame(deduped, columns=["Bus_Name", "Customer_Name", "Stars", "Comment", "Date"])

    # 1) L∆ØU FILE NG√ÄY (JSONL)
    _save_daily_jsonl(df)

    # 2) MERGE v√†o MASTER (JSONL) v·ªõi Review_Id tƒÉng d·∫ßn + lo·∫°i tr√πng
    _merge_to_master_jsonl(df)

    print("üéâ Reviews crawl done (saved daily JSONL + merged master JSONL).")


if __name__ == "__main__":
    crwl_reviews()